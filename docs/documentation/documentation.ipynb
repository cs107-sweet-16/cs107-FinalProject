{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30751743",
   "metadata": {},
   "source": [
    "<!-- autodiff documentation master file, created by\n",
    "sphinx-quickstart on Tue Nov  9 21:28:25 2021.\n",
    "You can adapt this file completely to your liking, but it should at least\n",
    "contain the root `toctree` directive. -->\n",
    "# Welcome to Autodiff’s Documentation!\n",
    "\n",
    "### Extended Documentation:\n",
    "\n",
    "* node module\n",
    "* Dualnumber module\n",
    "* Operatorsfunc module\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Differentiation is an important operation in scientific research broadly\n",
    "applied in non-linear optimization problems and numerical solution of\n",
    "differential equations. Recently, the development of machine learning\n",
    "techniques including artificial neural networks also require large amount of\n",
    "gradient computation. For complex functions whose analytical derivative\n",
    "expressions are difficult to obtain, one traditional option to calculate its\n",
    "derivative is finite-difference method, but may have issues such as\n",
    "floating point errors. Automatic differentiation (AD) refers to a general way\n",
    "of computing the derivatives of functions expressed as computation graphs, with\n",
    "which the evaluation of derivatives could reach machine precision. Our autodiff\n",
    "package provides tools to calculate the derivative of any function with an\n",
    "analytic expression based on the AD technique.\n",
    "\n",
    "# Background\n",
    "\n",
    "AD mainly makes use of two properties of closed-form functions:\n",
    "1. Any closed-form function is a combination (sum, product, quotient or compound) of\n",
    "elementary functions, and the analytical expressions of the derivatives of\n",
    "elementary functions are already-known.\n",
    "2. Derivative rules including sum rule, product rule, quotient rule and most importantly, the chain rule.\n",
    "- sum rule: $(f+g)’ = f’+g’$\n",
    "- product rule: $(f\\times g)’ = f’g+g’f\\times g$\n",
    "- quotient rule: $(f/g)’ = (f’g-g’f)/(g\\times g)$\n",
    "- chain rule: $h = f(g(•))$, then $h’ = f’(g(•))× g’(•)$\n",
    "\n",
    "To evaluate the value of a complicated function expression, we need to start with\n",
    "independent variables and evaluate a series of intermediate results and finally\n",
    "reach the final result. The idea of AD is to use a computational graph to\n",
    "represent the function where each node represents an intermediate variable and\n",
    "only elementary operations are carried out between intermediate variables. Because we\n",
    "utilize the operation rules of derivatives mentioned above, the derivative at any\n",
    "node in the computation graph can only rely on its parent node(s). As a result, we\n",
    "obtain the final derivative result by computing derivatives and repeatedly utilizing\n",
    "derivative rules following the flow of the computation graph.\n",
    "\n",
    "Here we give a brief example to illustrate how we calculate derivatives \n",
    "based on compution graph. If we want to evaluate the derivative with respect \n",
    "to x1 at x1=pi/2 for function f=sin(cos(x1)), here is our computation graph\n",
    "\n",
    "![comp graph](fig1.png)\n",
    "\n",
    "Then we follow the graph and evaluate step by step:\n",
    "\n",
    "|step| trace | Elementary function |Current value | Elementary function derivative| $\\nabla_x$ value|\n",
    "|--|--|--|--|-- |--|\n",
    "|1|v0|x|pi/2|$\\dot x$|1|\n",
    "|2|v1|cos(v0)|0|$-sin(v_0)\\dot v_0$|0|\n",
    "|3|f|sin(v1)|0|$cos(v_1)\\dot v_1$|0|\n",
    "\n",
    "Note that when we calculate \"elementary function derivative\", we made use of the chain rule. \n",
    "\n",
    "# How to use\n",
    "\n",
    "## Installation\n",
    "\n",
    "### Option 1:\n",
    "\n",
    "\n",
    "### Distribution \n",
    "We will used PyPI to distribute our package.   \n",
    "We will used a library called `twine` to help us package and distribute our\n",
    "package over PyPI. We will follow this helpful tutorial\n",
    "https://packaging.python.org/tutorials/packaging-projects/, provides a detailed\n",
    "guide on how to setup the structure. Using a framework may be helpful,\n",
    "but in an effort to reduce software overhead and maximize learning for each\n",
    "step, we will elect to forgo a framework. Users may install autodiff (?? is this the name) through the following command line:\n",
    "\n",
    "```\n",
    "pip3 install autodiff--aabj\n",
    "```\n",
    "\n",
    "Within a python terminal, you can then import the package:\n",
    "\n",
    "```\n",
    "import autodiff\n",
    "```\n",
    "\n",
    "Alternatively, to pull commmon functions directly you may use the following import statement:\n",
    "\n",
    "```\n",
    "from autodiff.node import variables, sin, cos, tan, exp, ln, log, sinh, cosh, tanh, sqrt, logistic, log_ab\n",
    "```\n",
    "\n",
    "### Option 2:\n",
    "\n",
    "In adition to pip installation, you may clone directly from the repository:\n",
    "\n",
    "```\n",
    "git clone https://github.com/cs107-sweet-16/cs107-FinalProject.git\n",
    "```\n",
    "\n",
    "After successful cloning of the repository, follow by installing the package requirements from the ‘requirements.txt’ file.\n",
    "\n",
    "```\n",
    "cd cs107-FinalProject\n",
    "pip3 install -r requirements.txt\n",
    "```\n",
    "\n",
    "Finally, finish by importing dualnumber and it’s operators\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "from node import sin, cos, tan, exp, log, valNode\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80469280",
   "metadata": {},
   "source": [
    "## Interaction\n",
    "\n",
    "Autodiff executes auto-differntiation by utilizting the concept of a computational graph. This graph is constructed during the execution of a forward pass. Simultaneously, the final value, and the respective gradient of each variable are also computed. Forward mode may be passed on some simple user-defined function, e.g. $$ f = 3x + 5^y $$. The computational graph is constructed via Python's native operator precedence and an ordered dictionaries. As each step is executed per Python's operator precedence, the respective values and derivatives are stored within the dictionary. The code for this may be found under the directory `autodiff/`\n",
    "\n",
    "### Workflow\n",
    "\n",
    "The general workflow for this project consists of the following:\n",
    "1. Determining some function of interest\n",
    "2. Initializing variables\n",
    "3. Defining the function\n",
    "4. Setting the variable values\n",
    "5. Executing forward mode or reverse mode to calculate the value of the function and derivatives\n",
    "The order of Step 3. and Step 4. can be switched, but the values of the variables must be set before the function is evaluated and differentiated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0611f17",
   "metadata": {},
   "source": [
    "### Multivariate Scalar Function Example\n",
    "\n",
    "When we use multiple variables to construct functions, we first declare the variables with `variable(name, size=None)` function, which accepts `name` as the names to identify the variables. When we declare scalar variables, we should not pass the `size` argument, or set `size=1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9711024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from autodiff.node import variables, sin, cos, tan, exp, ln, log, sinh, cosh, tanh, sqrt, logistic, log_ab, vector\n",
    "\n",
    "# EXAMPLE 1 for forward mode\n",
    "\n",
    "# testing cos(ab/c) + c*log(a), a = 4, b = -1, c = 10\n",
    "\n",
    "# initialize the nodes as variables\"\n",
    "a = variables('a')\n",
    "b = variables('b')\n",
    "c = variables('c')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44417b8a",
   "metadata": {},
   "source": [
    "\n",
    "Then we use the python operands or the elementary functions to construct the function expression, and set the values of the variables with `.set_val(val)` method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec38b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function:\n",
    "f = cos((a * b) / c) + c * log(a)\n",
    "\n",
    "# set the values of variables\n",
    "a.set_val(4)\n",
    "b.set_val(-1)\n",
    "c.set_val(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e90d33d",
   "metadata": {},
   "source": [
    "Then, we can use `.forward()` to evaluate the function and calculates its derivatives in forward-mode automatic differentiation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c6edadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of function f:  14.78400460520179\n",
      "gradient with respect to a:  2.461058165769135\n",
      "gradient with respect to b:  0.15576733692346023\n",
      "gradient with respect to c:  1.4018710948122366\n"
     ]
    }
   ],
   "source": [
    "# execute forward:\n",
    "f_val, f_grad = f.forward()\n",
    "\n",
    "\n",
    "# print the values of interest:\n",
    "print(\"value of function f: \", f_val)\n",
    "print(\"gradient with respect to a: \", f_grad['a'])\n",
    "print(\"gradient with respect to b: \", f_grad['b'])\n",
    "print(\"gradient with respect to c: \", f_grad['c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8938a239",
   "metadata": {},
   "source": [
    "Or we can use `.reverse()` instead to calculte the function value and derivatives in reverse-mode automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3db91625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of function f:  14.78400460520179\n",
      "gradient with respect to a:  2.461058165769135\n",
      "gradient with respect to b:  0.15576733692346023\n",
      "gradient with respect to c:  1.4018710948122366\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE 2 for reverse mode\n",
    "\n",
    "# execute reverse:\n",
    "f_val, f_grad = f.reverse()\n",
    "\n",
    "\n",
    "# print the values of interest:\n",
    "print(\"value of function f: \", f_val)\n",
    "print(\"gradient with respect to a: \", f_grad['a'])\n",
    "print(\"gradient with respect to b: \", f_grad['b'])\n",
    "print(\"gradient with respect to c: \", f_grad['c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d4348b",
   "metadata": {},
   "source": [
    "### Vector Function and Vector Input Example\n",
    "\n",
    "We can also define vector inputs and vector functions with `autodiff` package. To define a vector variable, call `variable(name, size)` with the name of the vector and the size of the vector variable. We can set the values of the vector variable by passing a list of values to `.set_val()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaafcf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXAMPLE 3 for vector function and vector input:\n",
    "# declare a as a vector\n",
    "a = variables('a', 3)\n",
    "# set a's values\n",
    "a.set_val([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c650bcf",
   "metadata": {},
   "source": [
    " To define a vector function, we need to use `vector` to vectorize all the output elements as a `vector` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30e66ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set functions:\n",
    "# f1(a) = a[0]**2\n",
    "# f2(a) = a[1]*a[2]\n",
    "# f = [f1, f2]\n",
    "# the output function need to be returned as a vector\n",
    "def func(a):\n",
    "    f1 = a[0]**2\n",
    "    f2 = a[1]*a[2]\n",
    "    return vector(f1, f2)\n",
    "f = func(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f1aa6",
   "metadata": {},
   "source": [
    "After the function is defined and constructed, we can use `.evaluate()` to evaluate the function and use `.grad()` to calculate the Jacobian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8585c114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of function f:  [1, 6]\n",
      "gradient of a at a=[1,2,3]:  [[2, 0, 0], [0, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# calculate function values\n",
    "f_val = f.evaluate()\n",
    "\n",
    "# calculate Jacobian matrix of the function\n",
    "f_grad = f.grad(a)\n",
    "\n",
    "# print the values of interest:\n",
    "print(\"value of function f: \", f_val)\n",
    "print(\"gradient of a at a=[1,2,3]: \", f_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4853dea6",
   "metadata": {},
   "source": [
    "\n",
    "### Simple Usage with Dual Numbers\n",
    "\n",
    "In addition to our node.py file, which contains code for forward and reverse passes on a given function, we also have more rudimentary code of `Dualnumber` class that constructs dual numbers and use forward-mode automatic differentiation to calculate the value and derivatives of functions. This `Dualnumber` class is more convenient to use when we have a single scalar input and scalar function. The files containing `Dualnumber` class are: `Dualnumber.py` and `Operatorsfunc.py`. Both of which can be found along within the `autodiff/` diretory.\n",
    "\n",
    "We can initialize a variable with `Dualnumber(val)`, which accepts an argument to be the value of the variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bd1572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from autodiff.dualnumber import Dualnumber\n",
    "from autodiff.operatorsfunc import tan, exp\n",
    "# set dual number 'x'\n",
    "# first value (np.pi) is the real part at which we want to calculate the derivative and value of the function\n",
    "# second value '1' is the dual part. In this case it is set to one, and will default to 1, but there is an option to change it. \n",
    "x = Dualnumber(np.pi, der=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1322e43e",
   "metadata": {},
   "source": [
    "Next, implement the function by substituting in the defined dual number variable. The value and derivative of the function is calculated during this process, and can be retrived by `.val` and `.der`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9652132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of function f:  -204.2160993874646\n",
      "gradient of x at x = pi:  -344.76791290283523\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Next implement your function, by substituting in the defined dual number:\n",
    "f = (tan(x)) - 2 ** x * exp(x)\n",
    "\n",
    "# The forward pass has now been executed and we can check the value of the function and it's derivative:\n",
    "\n",
    "print(\"value of function f: \", f.val)\n",
    "print(\"gradient of x at x = pi: \", f.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76428ef",
   "metadata": {},
   "source": [
    "## Software Organization\n",
    "\n",
    "### Directory Structure\n",
    "Our code will live in a separate folder called `src`. We will have a separate\n",
    "directory, `test`, for our tests. Finally, while not necessarily code, our\n",
    "documentation will live in a directory named `docs`, which will go into\n",
    "technical detail how each user-facing function is used, and any examples if\n",
    "appropriate.\n",
    "\n",
    "```\n",
    "CS107-FinalProject/\n",
    "  autodiff/\n",
    "    Dualnumber.py\n",
    "    Operatorsfunc.py\n",
    "    node.py\n",
    "  docs/\n",
    "    source/\n",
    "      ...\n",
    "    documentation.md\n",
    "    milestone1.md\n",
    "    milestone2.md\n",
    "    milestone2_progress.md\n",
    "  test/\n",
    "    __init__.py\n",
    "    test_node.py\n",
    "    test_dual.py\n",
    "    test_node_errs.py\n",
    "    test_operatorsfunc.py\n",
    "```\n",
    "\n",
    "### Modules\n",
    "\n",
    "We will have one package, named `autodiff`. Within this module there are a\n",
    "few modules, with the most core functionality in a module named `node.py`.  This\n",
    "module will provides us with the basic functionality of being able to\n",
    "differentiate on a defined set of elementary operations and functions. \n",
    "\n",
    "In addition to `node.py`, we also have `Dualnumber.py` and `Operatorsfunc.py` which has our dated implementation of autodiff in them. `Dualnumber.py` constructs a Dualnumber class, with native Python functions. `Operatorsfunc.py` consists of extended functions, such as sinusoidial, to extend the functionality of `Daulnumber.py`. \n",
    "\n",
    "### Test Suite\n",
    "\n",
    "Our test suite lives in a separate directory called `tests`. We will use\n",
    "TravisCI to run continuous integration, and make sure we do not accidentally\n",
    "introduce regressions in our code when we change or add functionality. CodeCov\n",
    "is used to ensure that any new code that we write is properly tested and\n",
    "accounted for.\n",
    "\n",
    "### Documentation\n",
    "We will use Sphinx as our documentation generator. To build sphinx, you can run the command inside\n",
    "the `docs/` directory:\n",
    "```\n",
    "sphinx-build source/ build/\n",
    "```\n",
    "The generated HTML files are viewable as `cs107-FinalProject/docs/build/index.html`.\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0bcac2",
   "metadata": {},
   "source": [
    "## Implementation \n",
    "We have a few core data structures:\n",
    "* Dual numbers\n",
    "* Binary-tree based computational graph\n",
    "\n",
    "### Dual Number for Simple Usage\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bda869",
   "metadata": {},
   "source": [
    "We first use dual number to implement basic forward mode for single variable scalar inputs. A dual number consists of a real part and a *dual* part, which is written as\n",
    "$$ z=a+b\\epsilon $$\n",
    "where $a,b\\in \\mathbb{R}$ and $\\epsilon$ is a special number such that $\\epsilon^2=0$ and $\\epsilon\\neq 0$.We notice that dual numbers have the following property\n",
    "$$ f(a+b\\epsilon) = f(a) + (f’(a)\\times b)\\epsilon $$\n",
    "For a univariate scalar function, it is convenient to use dual numbers to calculate its derivative this way. We overload python dunder methods for basic operands and defined other basic arithmetic functions with the `Dualnumber` class to automate this calculation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aca7d4a",
   "metadata": {},
   "source": [
    "\n",
    "#### class Dualnumber(a, der=1)\n",
    "We will begin by constructing a dualNumber() class that\n",
    "deals with the properties of dual numbers. It will take a real value and dual\n",
    "value as the initial input. The returns of calculations with dual numbers are also return dual numbers. Real numbers are considered as dual numbers with dual part=0.\n",
    "```py\n",
    "class Dualnumber:\n",
    "    def __init__(self, a, der=1):\n",
    "        self.val = a\n",
    "        self.der = der\n",
    "\n",
    "    def set_dual(self, dual):\n",
    "        self.der = dual\n",
    "\n",
    "    # dunder methods ...\n",
    "```\n",
    "\n",
    "#### Primitives\n",
    "The `Dualnumber` class supports the following operations and functions:\n",
    "* `+`(`__add__`,`__radd__`): add\n",
    "* `-`(`__sub__`,`__rsub__`): subtract\n",
    "* `*`(`__mul__`,`__rmul__`): multiply\n",
    "* `/`(`__truediv__`,`__rtruediv__`): truedivide\n",
    "* `**`(`__pow__`,`__rpow__`): power function\n",
    "* `-`(`__neg__`): standard unary negative operation\n",
    "* `+`(`__pos__`): standard unary positive operation\n",
    "* `sin`: sine function\n",
    "* `cos`: cosine function\n",
    "* `tan`: tangent function\n",
    "* `exp`: natural exponent\n",
    "* `log`: logarithm base e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0c24a",
   "metadata": {},
   "source": [
    "### Computational Graph\n",
    "#### Data Structure\n",
    "We build our computational graph based on binary tree data structure. The basic element of the graph is the node, which is represented as `Node` class in implementation. Each node in the graph can have at most two children.\n",
    "\n",
    "#### class node.Node()\n",
    "Bases: `object`\n",
    "\n",
    "`Node` is the basic element that constitutes the computational graph. `Node` supports unary and binary elementary functions, including native python functions such as `+-*/`, and elementary arithmetic functions such as `sin`, `cos` (all the elementary functions supported by `Node` is listed in *Primitive* section below). When an elementary function is called on `Node` instances, it will generate a new `Node` as the parent of the input `Node` instances, record the function or operation that should be implemented on the two input `Node`s in the new `Node`, and return the new `Node`. If a `Node` is calculated with a constant `float` or `int` input, the constant will be converted to a constant node in the computational graph. The returned `Node` can be considered as an intermediate variable in the computational graph. Therefore, during the process of using elementary functions on `Node`s to construct the complex function expression we want to calculate, we obtain a computational graph representing this function.\n",
    "\n",
    "#### class node.valNode(name=None)\n",
    "Bases: `node.Node`\n",
    "\n",
    "A `valNode` instance represents an input variable with respect to which we want to calculate derivatives, or a constant input. `valNode` should not have any child in the computational graph.\n",
    "* `valNode.name`: the name of the variable, which is used to distinguish different variables. `valNode.name` being `None` means it represents a constant input.\n",
    "* `valNode.val`: the value set for the variable, or the value of the constant.\n",
    "\n",
    "#### class node.funcNode(func, leftdf, rightdf)\n",
    "Bases: `node.Node`\n",
    "\n",
    "A `funcNode` instance represents an function that should be executed on one or two (independent or intermediate) variables in the computational graph. `funcNode` can represents both unary and binary functions.\n",
    "* `funcNode.left`: the first variable to be passed into the function\n",
    "* `funcNode.right`: the second variable to be passed into the function, should be `None` when the `funcNode` represents a unary function\n",
    "* `funcNode.func`: the arithmetic function to be implemented on the value of children nodes, could be either unary or binary\n",
    "* `funcNode.leftdf`: the arithmetic function which accepts the value of two variables and calculate the derivative of `funcNode.func` with respect to the first variable, should accept only one value if `funcNode.func` is a unary functio\n",
    "* `funcNode.rightdf`: the arithmetic function which accepts the value of two variables and calculate the derivative of `funcNode.func` with respect to the second variable, should be `None` if `funcNode.func` is a unary function\n",
    "\n",
    "\n",
    "#### Forward Mode\n",
    "The chain rule for forward-mode automatic differentiation is\n",
    "$$ v_3 = f(v_1, v_2) $$\n",
    "$$  \\nabla v_3 = \\frac{\\partial f}{\\partial v_1}\\nabla v_1 + \\frac{\\partial f}{\\partial v_2}\\nabla v_2  $$\n",
    "$v_1$, $v_2$, $\\nabla v_1$, and $\\nabla v_2$ must be calculated before calculating $v_3$ and $\\nabla v_3$. \n",
    "* `funcNode.forward()`: Executes a forward pass automatic differentiation. Recursively calculate the values and derivatives of left and right chidren, and use the calculation results to calculate self value and self derivatives. Keep track of independent variables with dictionary and the names of independent variables.\n",
    "\n",
    "#### Primitives\n",
    "Node also contains elementary operations. \n",
    "We will maintain an updated list of these operations:\n",
    "\n",
    "* `+`(`__add__`,`__radd__`): add\n",
    "* `-`(`__sub__`,`__rsub__`): subtract\n",
    "* `*`(`__mul__`,`__rmul__`): multiply\n",
    "* `/`(`__truediv__`,`__rtruediv__`): truedivide\n",
    "* `**`(`__pow__`,`__rpow__`): power function\n",
    "* `-`(`__neg__`): standard unary negative operation\n",
    "* `+`(`__pos__`): standard unary positive operation\n",
    "* `sin`: sine function\n",
    "* `cos`: cosine function\n",
    "* `tan`: tangent function\n",
    "* `sinh`: hyperbolic sine function\n",
    "* `cosh`: hyperbolic cosine function\n",
    "* `tanh`: hyperbolic tangent function\n",
    "* `arccos`: inverse cosine function\n",
    "* `arcsin`: inverse sine function\n",
    "* `arctan`: inverse tangent function\n",
    "* `exp`: natural exponent\n",
    "* `ln`: logarithm base e\n",
    "* `log_ab`: logarithm with arbitary base\n",
    "* `sqrt`: square root\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ff06ac",
   "metadata": {},
   "source": [
    "\n",
    "### Dependendies\n",
    "\n",
    "Our primary dependency within our working modules is `numpy`. `Numpy` was used to calculate values for non-native elementary functions, such as the sinusoidal classes. However we used numerous packages throughout our project. These include PyPi to form our project into a package, and codecove to determine code coverage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e4402",
   "metadata": {},
   "source": [
    "## Extension: Reverse Mode\n",
    "Besides forward-mode automatic differentiation supporting multiple input variables, we also implemented reverse-mode automatic differentiation with our computational graph structure. The reverse mode, similar to the forward mode, is also based on chain rule of derivatives, but it is more efficient than the the forward mode when the number of input variables are more than the number of outputs.\n",
    "\n",
    "### Implementation details\n",
    "The chain rule for reverse-mode automatic differentiation is\n",
    "$$\\bar{v_i} = \\frac{\\partial J}{\\partial v_i} = \\sum_{i\\ \\mathrm{a\\ child\\ of}\\ j} \\frac{\\partial v_j}{\\partial v_i} \\frac{\\partial J}{\\partial v_j} = \\sum_{i\\ \\mathrm{a\\ child\\ of}\\ j} \\frac{\\partial v_j}{\\partial v_i} \\bar{v_j}$$\n",
    "where $\\bar{v_i}= \\frac{\\partial J}{\\partial v_i}$ is the adjoint of $v_i$. The adjoint of an independent variable is equivalent to the derivative of $J$ with respect to this variable. In the reverse mode, the value of all intermediate variables need to be calculated before any derivatives and adjoints are calculated. The adjoints $\\bar{v_i}$ and partial derivatives $\\frac{\\partial v_j}{\\partial v_i}$ are passed top-to-bottom (from root to leaf) in the computational graph.\n",
    "\n",
    "* `funcNode.forward_pass()`: Execute a forward pass to calculate the variables of all intermediate variables in the computational graph.\n",
    "* `funcNode.reverse_pass()`: Executes a reverse pass to calculate the adjoints of all intermediate variables and independent variables. Recursively pass partial derivatives and adjoints to the children nodes.\n",
    "\n",
    "### Additional attributes\n",
    "* `funcNode.val`: store the value of intermediate variables calculated during the forward pass\n",
    "* `valNode.der`: store the adjoint of the `valNode` (i.e., the derivative) calculated during the reverse pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3740944",
   "metadata": {},
   "source": [
    "## Broader Impact and Inclusivity Statement\n",
    "\n",
    "### Broader Impact\n",
    "\n",
    "Automatic differentiation is used in many areas of scientific research and engineering. While autodifferentiation itself is not a new topic (and has been implemented many times), creating and building a autodifferentiation library is useful for understanding the concepts and elucidates how ubiquitously it can be used. Additionally, maintaining an open source library is good practice for understanding how to serve the software development community - writing clean, performant code, adhering to software development best practices, and being open to consider future extensions and migration of code standards.\n",
    "\n",
    "As in all math, part of the nature of autodifferentiation is that it can be used in both socially good and harmful ways. There are real-world consequences, for example, of using autodifferentiation to train a machine learning model for a specific purpose, particularly on user privacy. To that end, we encourage user feedback on the usage of the our library, and to open a conversation on its potential misuse.\n",
    "\n",
    "### Inclusitivity\n",
    "\n",
    "The authors of the autodiff library welcome everyone and encourage participation and discussion of how to use autodifferentiation, regardless of any social status, standing, or nature. These include (but are not limited to):\n",
    "\n",
    "age, culture, ethnicity, gender, natioanlity, politics, race, sex, sexual orientation, socioeconomic status This community fosters mutual respect and tolerance - while we cannot hope to be knowledgable about everything, we can hope to be better, and more informed than when we began. Our autodiff library is not meant to be rigid body, but rather an evolving piece of software that changes along with its community.\n",
    "\n",
    "Both contributors and core developers of the library are expected to hold each other accountable, and to open conversation with optimism, and not of expectation. To this end, we will have pull requests and issues reviewed in a thorough and unassuming manner.\n",
    "\n",
    "We encourage any and all underrepresented groups interested in software or learning more about how our code works to submit thoughtful tickets, raise issues, and ask for feature requests to help increase the inclusivity of our wording, and make it more accessible to experience and inexperienced Python practitioners alike, native and non-native English speakers as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f14279",
   "metadata": {},
   "source": [
    "## Future\n",
    "\n",
    "### Improving API\n",
    "In future, we would like to incorporate improved API for vector inputs. Currently, the user needs to define\n",
    "all functions for vector inputs. While this can't be done away in cases where there are multiple independent output functions f: $R^{n}$  → $R^{m}$, we can improve the user interface for cases where the output function is same for each of the output dimentions m. For example, currently calulating `sin(v)` for vector input `v` requires user to define a function for each output nodes which are all sine function. A better improved API could do away with that requirement.\n",
    "\n",
    "### Higher-Order Derivatives\n",
    "We would also like to support the calculation of ***Hessian*** function. Hessian, the second order derivative of a function f : $R^{n}$ → R, is a n x n square matrix **$(H_{f})_{i, j}$** = $\\dfrac{\\partial^2 f}{\\partial x_i\\partial x_j}$\n",
    "\n",
    "Hessian has many important application across various problems in science and engineering. For example, it is used in approximating functions as polynomial with higher precision in the Taylor Series expansion. Several interesting mathemetical property of functions like infelection points and determination of function curvature require calculating the second derivative of the function. Many optimaization functions in deep learning (for example, Newton's method) require computing Hessian function.\n",
    "\n",
    "To add Hessian function support in our package, we would need to change the existing implementation of reverse mode to support computational graph as an input to it. Hessian can be then calculated by a reverse mode applied on the compuational graph of the gradient calculate in the forward mode. In essence, this is equivalent to forward mode computation followed by reverse mode. Implementing Hessian is computationally expensive in terms of time and space requiremnts ($O(n^2)$ in both.) There are numerical methods to compute approximations for Hessians functions using quasi-Newton methods, that are more efficient.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
